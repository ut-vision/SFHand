# **SFHand: A Streaming Framework for Language-guided 3D Hand Forecasting and Embodied Manipulation**

[![Paper](https://img.shields.io/badge/Paper-B31B1B?style=for-the-badge\&logo=arxiv\&logoColor=white)](https://arxiv.org/pdf/2511.18127)
[![Data](https://img.shields.io/badge/Data-0040A1?style=for-the-badge\&logo=huggingface\&logoColor=ffffff)](https://huggingface.co/datasets/ut-vision/EgoHaFL)
[![Model](https://img.shields.io/badge/Model-FF6D00?style=for-the-badge\&logo=huggingface\&logoColor=ffffff)](https://huggingface.co/ut-vision/SFHand)

---

## üî• **Project Highlights**

![Demo GIF](assets/EgoHaFL.gif)

| Feature                      | Description                                      |
| ---------------------------- | ------------------------------------------------ |
| üì° **Streaming Framework**   | Autoregressive multi-modal hand forecasting      |
| ‚úã **Full-State Predictions** | Hand type, 2D box, 3D pose, and trajectory       |
| üß† **ROI-Enhanced Memory**     | Temporal hand awareness        |
| üó£Ô∏è **Language-guided**      | Follows natural language instructions            |
<!-- | ü§ñ **Embodied Ready**        | Downstream manipulation support (Franka Kitchen) | -->

---

## üé¨ **Method Overview**

![Method Figure](assets/method.jpg)

---

## üìù **Introduction**

> üí° **SFHand is the first streaming architecture for language-guided 3D hand forecasting.**

SFHand predicts future hand dynamics from continuous egocentric video + text instructions.
The model outputs the following hand states *autoregressively*: Hand type, 2D bounding box, 3D hand pose, and 3D trajectory

Key components: Streaming autoregressive transformer and ROI-enhanced memory.

---

## üì¶ **Project Status**

| Component                            | Status         |
| ------------------------------------ | -------------- |
| EgoHaFL Dataset                      | ‚úÖ              |
| Pretraining Code                     | ‚úÖ              |
| Pretrained Weights                   | ‚úÖ              |
| Evaluation Code                      | ‚úÖ              |
| Embodied Evaluation (Franka Kitchen) | üîú Coming soon |
| 3D Hand Annotation Code              | üîú Coming soon |

---

## üîß **Installation**
We develop and test the project under: `torch 2.8.0+cu129`.
```bash
git clone git@github.com:ut-vision/SFHand.git

conda env create -f environment.yml
conda activate sfhand

pip install -r requirements.txt
conda install -c conda-forge libgl
```

Download [MANO model](https://mano.is.tue.mpg.de/) and put `MANO_LEFT.pkl` and `MANO_RIGHT.pkl` under `data/mano`.

Download `base_best.pt` from [EgoHOD checkpoint](https://huggingface.co/Jazzcharles/EgoVideo) and place it at `./pre_ckpt/base_best.pt`.

---

## üìÇ **Dataset: EgoHaFL**

EgoHaFL Dataset (annotations)Ôºö
üëâ [https://huggingface.co/datasets/ut-vision/EgoHaFL](https://huggingface.co/datasets/ut-vision/EgoHaFL)

Videos originate from Ego4D V1: [https://ego4d-data.org/](https://ego4d-data.org/).
We use **224p compressed clips**.

Directory structure:

```
EgoHaFL
    ‚îú‚îÄ‚îÄ EgoHaFL_lmdb
    ‚îÇ   ‚îú‚îÄ‚îÄ data.mdb
    ‚îÇ   ‚îî‚îÄ‚îÄ lock.mdb
    ‚îú‚îÄ‚îÄ EgoHaFL_train.csv
    ‚îú‚îÄ‚îÄ EgoHaFL_test.csv
    ‚îî‚îÄ‚îÄ v1
        ‚îî‚îÄ‚îÄ videos_224p
```

---

## üöÄ **Training & Evaluation**

### **Train + Eval**

```bash
bash ./exps/pretrain.sh
```

> ‚ö†Ô∏è Before training, edit configs in `./configs`.

### **Eval + Visualization**

```bash
python main.py --config_file configs/config/clip_base_eval.yml --eval --vis
```

Output visualizations ‚Üí `./render_results/`

---

## üß† **Pretrained Models**

Download here:

üëâ [https://huggingface.co/ut-vision/SFHand](https://huggingface.co/ut-vision/SFHand)

---

## ü§ñ **Embodied Evaluation (Franka Kitchen)**

> ‚è≥ Coming soon ‚Äî code will be added once finalized.

---

## ‚úçÔ∏è **3D Hand Annotation**

> ‚è≥ Coming soon ‚Äî detailed annotation tools, formats, and processing scripts will be released once finalized.

---

## üìö **Citation**

```latex
@article{liu2025sfhand,
  title={SFHand: A Streaming Framework for Language-guided 3D Hand Forecasting and Embodied Manipulation},
  author={Liu, Ruicong and Huang, Yifei and Ouyang, Liangyang and Kang, Caixin and and Sato, Yoichi},
  journal={arXiv preprint arXiv:2511.18127},
  year={2025}
}
```

---

## üôè **Acknowledgement**

SFHand builds on **[EgoHOD](https://github.com/InternRobotics/EgoHOD)**.
Thanks to all contributors of the original codebase.

