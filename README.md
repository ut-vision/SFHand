# **SFHand: A Streaming Framework for Language-guided 3D Hand Forecasting and Embodied Manipulation**

[![Paper](https://img.shields.io/badge/Paper-B31B1B?style=for-the-badge\&logo=arxiv\&logoColor=white)](https://arxiv.org/pdf/2511.18127)
[![Data](https://img.shields.io/badge/Data-0040A1?style=for-the-badge\&logo=huggingface\&logoColor=ffffff)](https://huggingface.co/datasets/ut-vision/EgoHaFL)
[![Model](https://img.shields.io/badge/Model-FF6D00?style=for-the-badge\&logo=huggingface\&logoColor=ffffff)](https://huggingface.co/ut-vision/SFHand)

---

## ğŸ”¥ **Project Highlights**

| Feature                      | Description                                      |
| ---------------------------- | ------------------------------------------------ |
| ğŸ“¡ **Streaming Framework**   | Autoregressive multi-modal hand forecasting      |
| âœ‹ **Full-State Predictions** | Hand type, 2D box, 3D pose, and trajectory       |
| ğŸ§  **ROI-Enhanced Memory**     | Temporal hand awareness        |
| ğŸ—£ï¸ **Language-guided**      | Follows natural language instructions            |
<!-- | ğŸ¤– **Embodied Ready**        | Downstream manipulation support (Franka Kitchen) | -->

---

## ğŸ¬ **Method Overview**

![Method Figure](assets/method.jpg)

---

## ğŸ“ **Introduction**

> ğŸ’¡ **SFHand is the first streaming architecture for language-guided 3D hand forecasting.**

SFHand predicts future hand dynamics from continuous egocentric video + text instructions.
The model outputs the following hand states *autoregressively*: Hand type, 2D bounding box, 3D hand pose, and 3D trajectory

Key components: Streaming autoregressive transformer and ROI-enhanced memory.

---

## ğŸ“¦ **Project Status**

| Component                            | Status         |
| ------------------------------------ | -------------- |
| EgoHaFL Dataset                      | âœ…              |
| Pretraining Code                     | âœ…              |
| Pretrained Weights                   | âœ…              |
| Evaluation Code                      | âœ…              |
| Embodied Evaluation (Franka Kitchen) | ğŸ”œ Coming soon |

---

## ğŸ”§ **Installation**

```bash
git clone git@github.com:ut-vision/SFHand.git
conda env create -f environment.yml
conda activate sfhand
pip install -r requirements.txt
conda install -c conda-forge libgl
```

Download [MANO model](https://mano.is.tue.mpg.de/) and put `MANO_LEFT.pkl` and `MANO_RIGHT.pkl` under `data/mano`.

---

## ğŸ“‚ **Dataset: EgoHaFL**

EgoHaFL Dataset (annotations)ï¼š
ğŸ‘‰ [https://huggingface.co/datasets/ut-vision/EgoHaFL](https://huggingface.co/datasets/ut-vision/EgoHaFL)

Videos originate from Ego4D V1: [https://ego4d-data.org/](https://ego4d-data.org/).
We use **224p compressed clips**.

Directory structure:

```
EgoHaFL
    â”œâ”€â”€ EgoHaFL_lmdb
    â”‚   â”œâ”€â”€ data.mdb
    â”‚   â””â”€â”€ lock.mdb
    â”œâ”€â”€ EgoHaFL_train.csv
    â”œâ”€â”€ EgoHaFL_test.csv
    â””â”€â”€ v1
        â””â”€â”€ videos_224p
```

---

## ğŸš€ **Training & Evaluation**

### **Train + Eval**

```bash
bash ./exps/pretrain.sh
```

> âš ï¸ Before training, edit configs in `./configs`.

### **Eval + Visualization**

```bash
python main.py --config_file configs/config/clip_base_eval.yml --eval --vis
```

Output visualizations â†’ `./render_results/`

---

## ğŸ§  **Pretrained Models**

Download here:

ğŸ‘‰ [https://huggingface.co/ut-vision/SFHand](https://huggingface.co/ut-vision/SFHand)

---

## ğŸ¤– **Embodied Evaluation (Franka Kitchen)**

> â³ Coming soon â€” code will be added once finalized.

---

## ğŸ“š **Citation**

```latex
@article{liu2025sfhand,
  title={SFHand: A Streaming Framework for Language-guided 3D Hand Forecasting and Embodied Manipulation},
  author={Liu, Ruicong and Huang, Yifei and Ouyang, Liangyang and Kang, Caixin and and Sato, Yoichi},
  journal={arXiv preprint arXiv:2511.18127},
  year={2025}
}
```

---

## ğŸ™ **Acknowledgement**

SFHand builds on **[EgoHOD](https://github.com/InternRobotics/EgoHOD)**.
Thanks to all contributors of the original codebase.

