_base_: '../default_clip_base.yml'
data:
  dataset: EgoHaFL
  EgoHaFL_root: EgoHaFL/v1/videos_224p
  EgoHaFL_metadata: EgoHaFL/EgoHaFL_train.csv
  EgoHaFL_hand_root: EgoHaFL/EgoHaFL_lmdb

  clip_length: 8
  clip_stride: 16
  max_hand: 10
  depth_threshold: 2.5
  
train:
  batch_size: 256
  epochs: 5
  lr: 1e-4
  fix_lr: true
  save_freq: 1

model:
  name: CLIP_VITB16
  freeze_temperature: true
  lavila_path: dont_use
  clip_length: 4 # This is defined by pre-trained EgoHOD weights, NOT CHANGEABLE
  pred_clip_length: 1 # Number of frames to predict
  max_hand: 10
  memory_capacity: 1386

loss:
  set_cost_class: 0
  set_cost_bbox: 10
  set_cost_giou: 4
  pose_loss_coef: 2
  bbox_loss_coef: 5
  giou_loss_coef: 2
  itc_loss_coef: 1
  eos_coef: 0.1
wandb: true
resume: pre_ckpt/base_best.pt

MANO:
  MODEL_PATH: data/mano
  NUM_HAND_JOINTS: 15

output_dir: debug

