_base_: '../default_clip_base.yml'
data:
  dataset: EgoHaFL
  EgoHaFL_root: EgoHaFL/v1/videos_224p
  EgoHaFL_metadata: EgoHaFL/EgoHaFL_test.csv
  EgoHaFL_hand_root: EgoHaFL/EgoHaFL_lmdb

  clip_length: 8
  clip_stride: 16
  max_hand: 10
  depth_threshold: 2.5

test:
  batch_size: 64

model:
  name: CLIP_VITB16
  freeze_temperature: true
  lavila_path: dont_use
  clip_length: 4 # This is defined by pre-trained EgoHOD weights, NOT CHANGEABLE
  pred_clip_length: 1 # Number of frames to predict
  max_hand: 10

loss:
  set_cost_class: 0
  set_cost_bbox: 10
  set_cost_giou: 4
  pose_loss_coef: 2
  bbox_loss_coef: 5
  giou_loss_coef: 2
  itc_loss_coef: 1
  eos_coef: 0.1
wandb: true
resume: debug/full_train/baseline_8gpu_incr/checkpoint_epoch5.pt

MANO:
  MODEL_PATH: data/mano
  NUM_HAND_JOINTS: 15

output_dir: debug

