data:
  dataset: EgoHaFL
  type: normal
  EgoHaFL_root: EgoHaFL/v1/videos_224p
  EgoHaFL_metadata: EgoHaFL/EgoHaFL_train.csv
  EgoHaFL_metadata_aux: null
  EgoHaFL_video_chunk_len: -1
  EgoHaFL_fps: 30

  clip_length: 16
  clip_stride: 4
  input_size: 224
  patch_size: 16
  is_trimmed: true

  context_length: 77 # 128
  vocab_size: 49408
  norm_style: clip
  fused_decode_crop: true
  decode_threads: 1
  
  multiview: false
  clear_narration: false
  return_uid: false
  
model:
  name: CLIP_VITB16
  # norm_embed: true
  clip_length: ${data.clip_length}
  # contrastive_use_vissl: true   #use contrastive implementation in vissl
  temperature_init: 0.07
  
  freeze_temperature: true
  grad_checkpointing: true
  use_fast_conv1: true
  use_flash_attn: true
  patch_dropout: 0.0
  drop_path_rate: 0.0
  pretrain_zoo: intern
  pretrain_path: null
  project_embed_dim: 512

  multiview: ${data.multiview}

train:
  task: vlp
  batch_size: 16
  epochs: 10
  warmup_epochs: 1
  lr: 1e-5
  fix_lr: true
  lr_start: 6e-8
  lr_end: 6e-7
  grad_clip_norm: null
  update_freq: 1
  seed: 0
  workers: 10

  optimizer:
    name: adamw
    wd: 0.01
    betas: [0.9, 0.999]
    eps: 1e-8

  eval_freq: 1
  print_freq: 10
  save_freq: 1
  disable_amp: false

  use_half: false
  find_unused_parameters: false

  local_loss: false
  gather_with_grad: false
  use_zero: false

  use_multi_epochs_loader: false

test:
  batch_size: 16
  workers: 10
  testonly: false
  savemetaonly: false

wandb: false
resume: null
resume_pretrain: null
output_dir: output_dir/debug/
local_rank: 0

visualisation: false

